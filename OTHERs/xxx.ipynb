{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Banking Data Analysis — PySpark notebook\n",
    "# Author: ChatGPT (adapt for your environment)\n",
    "# Usage: Run in PySpark environment (pyspark shell, Databricks, EMR, or local with pyspark installed).\n",
    "# Assumptions:\n",
    "#  - exchange_rates.csv exists locally or at specified path (columns: currency, rate_to_USD).\n",
    "#  - AWS credentials configured via environment / IAM role if using S3.\n",
    "#  - Python packages: pandas, matplotlib, seaborn, sqlite3 (standard), sqlalchemy (optional).\n",
    "\n",
    "# ---------- 0. Imports & SparkSession ----------\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine   # optional, for SQLite write via pandas\n",
    "\n",
    "# overall comment:\n",
    "# Create SparkSession. For S3 writes, ensure hadoop-aws / aws Java libs present when using s3a://\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BankingDataAnalysis\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ---------- 1. Data Acquisition (scrape archived Wikipedia) ----------\n",
    "# We'll use pandas to read the archived wiki HTML table and then convert to Spark DataFrame.\n",
    "wiki_url = \"https://web.archive.org/web/20230908091635/https://en.wikipedia.org/wiki/List_of_largest_banks\"\n",
    "\n",
    "# overall comment:\n",
    "# Pandas' read_html is convenient to pull the table. If network disabled, download the HTML manually and point to local file.\n",
    "tables = pd.read_html(wiki_url)  # may return multiple tables\n",
    "\n",
    "# Inspect and pick the correct table\n",
    "# overall comment: print shapes to choose the right table (usually the 'Market cap' table).\n",
    "for i, t in enumerate(tables):\n",
    "    print(i, t.shape, t.columns.tolist())\n",
    "\n",
    "# Assume the correct table is the first or find by column name\n",
    "# Example: columns might be ['Rank', 'Bank', 'Market capitalization (USD billion)', ...]\n",
    "# We'll attempt to locate the table containing 'Market' in column names:\n",
    "table_idx = None\n",
    "for i, t in enumerate(tables):\n",
    "    cols = \" \".join([str(c).lower() for c in t.columns])\n",
    "    if \"market\" in cols or \"market cap\" in cols or \"market capitalization\" in cols:\n",
    "        table_idx = i\n",
    "        break\n",
    "if table_idx is None:\n",
    "    # fallback\n",
    "    table_idx = 0\n",
    "\n",
    "df_raw = tables[table_idx].copy()\n",
    "print(\"Using table index:\", table_idx)\n",
    "df_raw.head(5)\n",
    "\n",
    "# ---------- 2. Quick pandas cleaning to normalize column names ----------\n",
    "# Normalize column names\n",
    "df_raw.columns = [str(c).strip() for c in df_raw.columns]\n",
    "# Try to standardize to: Rank, Bank Name, Market Cap (USD Billion)\n",
    "# We'll attempt to find the right market-cap column by name fuzzy match\n",
    "market_col = None\n",
    "for c in df_raw.columns:\n",
    "    lc = c.lower()\n",
    "    if \"market\" in lc and (\"usd\" in lc or \"cap\" in lc or \"capital\" in lc):\n",
    "        market_col = c\n",
    "        break\n",
    "if market_col is None:\n",
    "    # fallback to last column\n",
    "    market_col = df_raw.columns[-1]\n",
    "\n",
    "# rename columns\n",
    "renames = {\n",
    "    df_raw.columns[0]: \"Rank\",\n",
    "    df_raw.columns[1]: \"Bank Name\",\n",
    "    market_col: \"MarketCap_USD_Billion\"\n",
    "}\n",
    "df = df_raw.rename(columns=renames)\n",
    "df = df[[\"Rank\", \"Bank Name\", \"MarketCap_USD_Billion\"]]\n",
    "\n",
    "# ---------- 3. Data Preparation & Cleaning (pandas) ----------\n",
    "# Overall approach:\n",
    "# - Remove rows where Bank Name or MarketCap is missing.\n",
    "# - Clean MarketCap column: remove commas, footnotes, non-numeric chars, convert to float (billions).\n",
    "# - Convert Rank to integer where possible.\n",
    "\n",
    "def clean_market_value(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x)\n",
    "    # remove references like [1], commas, dollar signs, parentheses text\n",
    "    import re\n",
    "    s = re.sub(r\"\\[.*?\\]\", \"\", s)           # remove bracketed footnotes\n",
    "    s = s.replace(\",\", \"\")\n",
    "    s = s.replace(\"US$\", \"\")\n",
    "    s = s.replace(\"$\", \"\")\n",
    "    s = s.strip()\n",
    "    # Sometimes market cap has '—' or 'N/A' or ranges; handle gracefully\n",
    "    if s in [\"—\", \"-\", \"N/A\", \"\"]:\n",
    "        return np.nan\n",
    "    # extract first numeric token\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", s)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group())\n",
    "        except:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "df[\"MarketCap_USD_Billion\"] = df[\"MarketCap_USD_Billion\"].apply(clean_market_value)\n",
    "# Rank cleaning\n",
    "def clean_rank(x):\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return None\n",
    "        s = str(x).strip()\n",
    "        s = s.split()[0]\n",
    "        return int(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df[\"Rank\"] = df[\"Rank\"].apply(clean_rank)\n",
    "# Drop rows missing bank name or marketcap\n",
    "df = df.dropna(subset=[\"Bank Name\", \"MarketCap_USD_Billion\"])\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"Rows after cleaning:\", df.shape[0])\n",
    "df.head()\n",
    "\n",
    "# ---------- 4. Move to Spark DataFrame ----------\n",
    "spark_df = spark.createDataFrame(df)\n",
    "# enforce schema and types\n",
    "schema_df = spark_df.withColumn(\"Rank\", F.col(\"Rank\").cast(T.IntegerType())) \\\n",
    "    .withColumn(\"MarketCap_USD_Billion\", F.col(\"MarketCap_USD_Billion\").cast(T.DoubleType())) \\\n",
    "    .withColumn(\"BankName\", F.col(\"Bank Name\")).drop(\"Bank Name\")\n",
    "\n",
    "# reorder\n",
    "spark_df = schema_df.select(\"Rank\", \"BankName\", \"MarketCap_USD_Billion\")\n",
    "spark_df.printSchema()\n",
    "spark_df.show(5, truncate=False)\n",
    "\n",
    "# ---------- 5. Handling Missing Values (report) ----------\n",
    "# Approach for missing values:\n",
    "# - small number of missing Rank -> leave as NULL or recompute ranking based on MarketCap.\n",
    "# - missing MarketCap -> drop (can't analyze) or mark as unknown. Here we dropped rows missing market cap.\n",
    "missing_counts = {c: spark_df.filter(F.col(c).isNull()).count() for c in spark_df.columns}\n",
    "print(\"Missing counts:\", missing_counts)\n",
    "\n",
    "# ---------- 6. Outliers — identification ----------\n",
    "# We'll compute z-scores or IQR in Spark (convert to pandas to compute robustly)\n",
    "pd_df_for_stats = spark_df.select(\"MarketCap_USD_Billion\").toPandas()\n",
    "q1 = pd_df_for_stats[\"MarketCap_USD_Billion\"].quantile(0.25)\n",
    "q3 = pd_df_for_stats[\"MarketCap_USD_Billion\"].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower = q1 - 1.5 * iqr\n",
    "upper = q3 + 1.5 * iqr\n",
    "print(\"IQR outlier bounds:\", lower, upper)\n",
    "# Outliers are banks above 'upper' (likely very large global banks). We'll keep them but note them.\n",
    "\n",
    "# ---------- 7. Exchange rates (load CSV) ----------\n",
    "# exchange_rates.csv format assumption:\n",
    "# currency, rate_to_USD\n",
    "# e.g. USD, 1.0; GBP, 1.23; EUR, 1.07; INR, 0.012 (example: 1 INR = 0.012 USD)\n",
    "exchange_rates_csv_path = \"./exchange_rates.csv\"\n",
    "try:\n",
    "    exch_pd = pd.read_csv(exchange_rates_csv_path)\n",
    "except Exception as e:\n",
    "    print(\"Failed to read exchange_rates.csv:\", e)\n",
    "    # fallback sample rates (you should replace with actual CSV)\n",
    "    exch_pd = pd.DataFrame({\n",
    "        \"currency\": [\"USD\", \"GBP\", \"EUR\", \"INR\"],\n",
    "        \"rate_to_USD\": [1.0, 1.25, 1.10, 0.012]   # example: 1 INR = 0.012 USD\n",
    "    })\n",
    "exch_pd\n",
    "\n",
    "# Convert to mapping: USD to other currencies -> compute multiplier to convert USD -> target currency\n",
    "# If rate_to_USD is how much 1 target currency equals USD, we want USD -> target = 1 / rate_to_USD\n",
    "# But naming varies; assume rate_to_USD means 1 unit of currency = rate_to_USD USD\n",
    "rate_map = {}\n",
    "for _, r in exch_pd.iterrows():\n",
    "    cur = r['currency'].strip().upper()\n",
    "    to_usd = float(r['rate_to_USD'])\n",
    "    if to_usd == 0:\n",
    "        rate_map[cur] = None\n",
    "    else:\n",
    "        rate_map[cur] = 1.0 / to_usd  # multiply USD value by this to get currency amount\n",
    "rate_map\n",
    "\n",
    "# ---------- 8. Add converted columns to Spark DF ----------\n",
    "# MarketCap_USD_Billion is in USD billions. We'll compute amounts in target currencies (billions).\n",
    "def add_currency_columns(sdf, rate_map):\n",
    "    sdf2 = sdf\n",
    "    for cur, mult in rate_map.items():\n",
    "        if mult is None:\n",
    "            continue\n",
    "        colname = f\"MarketCap_{cur}_Billion\"\n",
    "        # multiply USD value by conversion multiplier\n",
    "        sdf2 = sdf2.withColumn(colname, F.col(\"MarketCap_USD_Billion\") * F.lit(mult))\n",
    "    return sdf2\n",
    "\n",
    "spark_df = add_currency_columns(spark_df, rate_map)\n",
    "spark_df.show(5, truncate=False)\n",
    "\n",
    "# ---------- 9. EDA — convert to pandas for plotting ----------\n",
    "pandas_df = spark_df.toPandas().sort_values(by=\"MarketCap_USD_Billion\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 3.2 Distribution histogram\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(pandas_df[\"MarketCap_USD_Billion\"], bins=20, kde=True)\n",
    "plt.title(\"Distribution of Market Cap (USD Billion)\")\n",
    "plt.xlabel(\"MarketCap (USD Billion)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.3 Top 10 banks by marketcap bar chart\n",
    "top10 = pandas_df.head(10).copy()\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=top10, x=\"MarketCap_USD_Billion\", y=\"BankName\")\n",
    "plt.title(\"Top 10 Banks by Market Cap (USD Billion)\")\n",
    "plt.xlabel(\"MarketCap (USD Billion)\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.4 Scatter: marketcap vs rank\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.scatterplot(data=pandas_df, x=\"Rank\", y=\"MarketCap_USD_Billion\")\n",
    "plt.title(\"Rank vs Market Cap (USD Billion)\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"MarketCap (USD Billion)\")\n",
    "plt.gca().invert_xaxis()  # smaller rank = top; invert if you prefer\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.5 Boxplot for spread & outliers\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x=pandas_df[\"MarketCap_USD_Billion\"])\n",
    "plt.title(\"Boxplot - Market Cap (USD Billion)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.6 Violin plot quartile distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.violinplot(x=pandas_df[\"MarketCap_USD_Billion\"], inner=\"quartile\")\n",
    "plt.title(\"Violin Plot - Market Cap (USD Billion)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.7 Cumulative market share line plot\n",
    "pandas_df[\"MarketCap_USD_Billion_sorted\"] = pandas_df[\"MarketCap_USD_Billion\"].sort_values(ascending=False).values\n",
    "pandas_df[\"CumulativeSum\"] = pandas_df[\"MarketCap_USD_Billion_sorted\"].cumsum()\n",
    "total = pandas_df[\"MarketCap_USD_Billion\"].sum()\n",
    "pandas_df[\"CumulativeSharePct\"] = pandas_df[\"CumulativeSum\"] / total * 100\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(pandas_df)+1), pandas_df[\"CumulativeSharePct\"], marker='o')\n",
    "plt.title(\"Cumulative Market Share of Banks (%)\")\n",
    "plt.xlabel(\"Number of Banks (sorted by Market Cap descending)\")\n",
    "plt.ylabel(\"Cumulative Share (%)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.8 Categorize banks into ranges (micro, small, mid, large, mega) — you can tune thresholds\n",
    "bins = [0, 10, 50, 200, 500, 5000]  # in USD Billion\n",
    "labels = [\"<10B\", \"10-50B\", \"50-200B\", \"200-500B\", \">500B\"]\n",
    "pandas_df[\"CapCategory\"] = pd.cut(pandas_df[\"MarketCap_USD_Billion\"], bins=bins, labels=labels, include_lowest=True)\n",
    "cat_counts = pandas_df[\"CapCategory\"].value_counts().reindex(labels).fillna(0)\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(x=cat_counts.index, y=cat_counts.values)\n",
    "plt.title(\"Distribution across Market Cap Ranges\")\n",
    "plt.xlabel(\"Market Cap Range (USD Billion)\")\n",
    "plt.ylabel(\"Number of Banks\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3.9 Pie chart of top 10 market share\n",
    "plt.figure(figsize=(7,7))\n",
    "top10 = pandas_df.head(10)\n",
    "plt.pie(top10[\"MarketCap_USD_Billion\"], labels=top10[\"BankName\"], autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Market Share Distribution (Top 10 Banks)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- 10. Advanced ETL / Querying tasks (Spark SQL + DataFrame APIs) ----------\n",
    "spark_df.createOrReplaceTempView(\"Largest_banks\")\n",
    "\n",
    "# 4.1 Advanced Market Cap Analysis with Growth Metrics (if we had historical series)\n",
    "# For this dataset (single snapshot), we can create \"growth-like\" metrics relative to next/lower bank:\n",
    "# Example: pct_diff_to_next = (marketcap - next_marketcap)/next_marketcap\n",
    "df_window = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  *,\n",
    "  LEAD(MarketCap_USD_Billion) OVER (ORDER BY MarketCap_USD_Billion DESC) as Next_MarketCap_USD_Billion\n",
    "FROM Largest_banks\n",
    "ORDER BY MarketCap_USD_Billion DESC\n",
    "\"\"\")\n",
    "df_growth = df_window.withColumn(\n",
    "    \"PctDiff_to_Next\",\n",
    "    F.when(F.col(\"Next_MarketCap_USD_Billion\").isNotNull(),\n",
    "           (F.col(\"MarketCap_USD_Billion\") - F.col(\"Next_MarketCap_USD_Billion\")) / F.col(\"Next_MarketCap_USD_Billion\") * 100\n",
    "          ).otherwise(F.lit(None))\n",
    ")\n",
    "df_growth.select(\"Rank\", \"BankName\", \"MarketCap_USD_Billion\", \"Next_MarketCap_USD_Billion\", \"PctDiff_to_Next\").show(20, truncate=False)\n",
    "\n",
    "# 4.2 Market Concentration & Categorize based on Market Share Tiers\n",
    "# compute each bank's share\n",
    "total_marketcap = spark_df.agg(F.sum(\"MarketCap_USD_Billion\").alias(\"total\")).collect()[0][\"total\"]\n",
    "market_share_df = spark_df.withColumn(\"MarketSharePct\", F.col(\"MarketCap_USD_Billion\")/F.lit(total_marketcap)*100)\n",
    "# define tiering\n",
    "market_share_df = market_share_df.withColumn(\n",
    "    \"Tier\",\n",
    "    F.when(F.col(\"MarketSharePct\") >= 10, F.lit(\"Mega (>10%)\"))\n",
    "     .when(F.col(\"MarketSharePct\") >= 5, F.lit(\"Large (5-10%)\"))\n",
    "     .when(F.col(\"MarketSharePct\") >= 1, F.lit(\"Mid (1-5%)\"))\n",
    "     .otherwise(F.lit(\"Small (<1%)\"))\n",
    ")\n",
    "market_share_df.select(\"Rank\", \"BankName\", \"MarketCap_USD_Billion\", \"MarketSharePct\", \"Tier\").orderBy(F.desc(\"MarketCap_USD_Billion\")).show(20, truncate=False)\n",
    "\n",
    "# 4.3 Quartile analysis (compute quartiles & assign quartile)\n",
    "quantiles = spark_df.approxQuantile(\"MarketCap_USD_Billion\", [0.25, 0.5, 0.75], 0.01)\n",
    "q1, q2, q3 = quantiles\n",
    "print(\"Quartiles (USD Billion):\", q1, q2, q3)\n",
    "quartile_df = spark_df.withColumn(\n",
    "    \"Quartile\",\n",
    "    F.when(F.col(\"MarketCap_USD_Billion\") <= q1, F.lit(\"Q1\"))\n",
    "     .when((F.col(\"MarketCap_USD_Billion\") > q1) & (F.col(\"MarketCap_USD_Billion\") <= q2), F.lit(\"Q2\"))\n",
    "     .when((F.col(\"MarketCap_USD_Billion\") > q2) & (F.col(\"MarketCap_USD_Billion\") <= q3), F.lit(\"Q3\"))\n",
    "     .otherwise(F.lit(\"Q4\"))\n",
    ")\n",
    "quartile_df.select(\"Rank\", \"BankName\", \"MarketCap_USD_Billion\", \"Quartile\").orderBy(F.desc(\"MarketCap_USD_Billion\")).show(20, truncate=False)\n",
    "\n",
    "# 4.4 Comparative size analysis (classify relative market size vs median)\n",
    "median_val = q2\n",
    "size_df = spark_df.withColumn(\n",
    "    \"RelativeSize\",\n",
    "    F.when(F.col(\"MarketCap_USD_Billion\") >= median_val * 2, F.lit(\"Much Larger\"))\n",
    "     .when(F.col(\"MarketCap_USD_Billion\") >= median_val * 1.1, F.lit(\"Larger\"))\n",
    "     .when(F.col(\"MarketCap_USD_Billion\") >= median_val * 0.9, F.lit(\"Similar\"))\n",
    "     .when(F.col(\"MarketCap_USD_Billion\") >= median_val * 0.5, F.lit(\"Smaller\"))\n",
    "     .otherwise(F.lit(\"Much Smaller\"))\n",
    ")\n",
    "size_df.select(\"Rank\", \"BankName\", \"MarketCap_USD_Billion\", \"RelativeSize\").orderBy(F.desc(\"MarketCap_USD_Billion\")).show(20, truncate=False)\n",
    "\n",
    "# 4.5 Evaluate Market Growth & Gaps between consecutive banks (we already computed PctDiff_to_Next)\n",
    "df_growth.select(\"Rank\", \"BankName\", \"MarketCap_USD_Billion\", \"Next_MarketCap_USD_Billion\", \"PctDiff_to_Next\").orderBy(F.desc(\"PctDiff_to_Next\")).show(20, truncate=False)\n",
    "\n",
    "# 4.6 Market Dominance score: e.g., dominance = market_share * log(marketcap) or Herfindahl-like\n",
    "# Simple dominance: market_share_pct * log10(marketcap + 1)\n",
    "market_dominance_df = market_share_df.withColumn(\"DominanceScore\", F.col(\"MarketSharePct\") * F.log10(F.col(\"MarketCap_USD_Billion\") + F.lit(1)))\n",
    "market_dominance_df.select(\"Rank\", \"BankName\", \"MarketCap_USD_Billion\", \"MarketSharePct\", \"DominanceScore\").orderBy(F.desc(\"DominanceScore\")).show(20, truncate=False)\n",
    "\n",
    "# 4.7 Segment-wise performance (by CapCategory previously computed in pandas; we can recompute)\n",
    "# Convert bins to Spark by using percentile-based buckets or the same bins\n",
    "bins_broadcast = bins\n",
    "# We'll join pandas category back for simplicity:\n",
    "pandas_df_for_join = pandas_df[[\"BankName\", \"CapCategory\"]]\n",
    "spark_cat_df = spark.createDataFrame(pandas_df_for_join)\n",
    "spark_full = spark_df.join(spark_cat_df, on=\"BankName\", how=\"left\")\n",
    "spark_full.groupBy(\"CapCategory\").agg(\n",
    "    F.count(\"*\").alias(\"Count\"),\n",
    "    F.sum(\"MarketCap_USD_Billion\").alias(\"TotalMarketCap\"),\n",
    "    F.avg(\"MarketCap_USD_Billion\").alias(\"AvgMarketCap\")\n",
    ").orderBy(\"CapCategory\").show(truncate=False)\n",
    "\n",
    "# 4.8 Create dashboard-ready aggregated table (top metrics)\n",
    "dashboard_df = market_share_df.select(\n",
    "    \"Rank\", \"BankName\", \"MarketCap_USD_Billion\", \"MarketSharePct\", \"Tier\"\n",
    ").orderBy(F.desc(\"MarketCap_USD_Billion\"))\n",
    "dashboard_df.show(20, truncate=False)\n",
    "\n",
    "# ---------- 11. Save outputs (CSV and SQLite) ----------\n",
    "output_folder_local = \"./output\"\n",
    "import os\n",
    "os.makedirs(output_folder_local, exist_ok=True)\n",
    "\n",
    "# Save CSV from Spark\n",
    "csv_out_path = os.path.join(output_folder_local, \"largest_banks_processed.csv\")\n",
    "spark_df.toPandas().to_csv(csv_out_path, index=False)\n",
    "print(\"Saved CSV to\", csv_out_path)\n",
    "\n",
    "# Save to SQLite (single table)\n",
    "sqlite_path = os.path.join(output_folder_local, \"largest_banks.db\")\n",
    "engine = create_engine(f\"sqlite:///{sqlite_path}\")\n",
    "# convert spark df to pandas then to sqlite\n",
    "spark_df.toPandas().to_sql(\"Largest_banks\", con=engine, if_exists=\"replace\", index=False)\n",
    "print(\"Saved SQLite DB to\", sqlite_path)\n",
    "\n",
    "# ---------- 12. Upload to S3 (if desired) ----------\n",
    "# Preferred: use IAM role on cluster, or AWS CLI-configured credentials in environment. Example using boto3:\n",
    "import boto3\n",
    "s3 = boto3.client(\"s3\")   # will use env credentials or role\n",
    "bucket_name = \"your-s3-bucket-name\"   # replace\n",
    "s3_key = \"banking-analysis/largest_banks_processed.csv\"\n",
    "try:\n",
    "    s3.upload_file(csv_out_path, bucket_name, s3_key)\n",
    "    print(f\"Uploaded to s3://{bucket_name}/{s3_key}\")\n",
    "except Exception as e:\n",
    "    print(\"S3 upload skipped or failed (configure credentials/role). Error:\", e)\n",
    "\n",
    "# If you prefer Spark write to S3 (s3a://), configure hadoop properties and write:\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIA...\")\n",
    "# spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"SECRET\")\n",
    "# spark_df.write.option(\"header\", True).csv(\"s3a://your-bucket/path/largest_banks_processed.csv\", mode=\"overwrite\")\n",
    "\n",
    "# ---------- 13. Save summary metrics to a small JSON (optional) ----------\n",
    "summary = {\n",
    "    \"total_banks\": int(spark_df.count()),\n",
    "    \"total_marketcap_usd_billion\": float(spark_df.agg(F.sum(\"MarketCap_USD_Billion\")).collect()[0][0]),\n",
    "    \"top_bank\": spark_df.orderBy(F.desc(\"MarketCap_USD_Billion\")).select(\"BankName\").first()[0]\n",
    "}\n",
    "import json\n",
    "with open(os.path.join(output_folder_local, \"summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Saved summary.json\")\n",
    "\n",
    "# ---------- 14. Final notes ----------\n",
    "# Stop SparkSession if done\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
